{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40fd061-e426-469d-a21e-9395863243af",
   "metadata": {},
   "source": [
    "# Dev of HalConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a24f0-7f6f-4b98-96bd-e53de58b595d",
   "metadata": {},
   "source": [
    "## General setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc476a-3839-4820-93da-97df3281bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of merge_pub_employees module\n",
    "\n",
    "# Standard Library imports\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# 3rd party imports\n",
    "import BiblioParsing as bp\n",
    "import pandas as pd\n",
    "\n",
    "# Local imports\n",
    "import bmfuncts.employees_globals as eg\n",
    "import bmfuncts.pub_globals as pg\n",
    "from bmfuncts.config_utils import set_user_config\n",
    "from bmfuncts.useful_functs import read_parsing_dict\n",
    "from bmfuncts.rename_cols import build_col_conversion_dic\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffaaac-d89c-47b9-bd38-4e15d32c0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working parameters\n",
    "\n",
    "# Setting working folder\n",
    "institute = \"Liten\"\n",
    "app_path = Path(r\"C:\\Users\\AC265100\\Documents\\BiblioMeter_App\") / Path(institute.upper())\n",
    "hal_path = app_path / Path(\"ConfMeter_Files\")\n",
    "\n",
    "# Setting corpus year\n",
    "year_select = \"2024\"\n",
    "year_hal_path = hal_path / Path(year_select)\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae644c23-3c45-4e18-8bc2-57f56b405897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employees common parameters (independant of year_select)\n",
    "\n",
    "# Setting specific aliases\n",
    "effectifs_root_alias = eg.EMPLOYEES_ARCHI[\"root\"]\n",
    "effectifs_folder_name_alias = eg.EMPLOYEES_ARCHI[\"all_years_employees\"]\n",
    "effectifs_file_name_alias = eg.EMPLOYEES_ARCHI[\"employees_file_name\"]\n",
    "search_depth_init_alias = eg.SEARCH_DEPTH\n",
    "first_name_col_alias = eg.EMPLOYEES_USEFUL_COLS['first_name']\n",
    "last_name_col_alias = eg.EMPLOYEES_USEFUL_COLS['name']\n",
    "full_name_col_alias = eg.EMPLOYEES_ADD_COLS['employee_full_name']\n",
    "\n",
    "# Setting useful paths independent from year_select\n",
    "effectifs_root_path   = hal_path / Path(effectifs_root_alias)\n",
    "effectifs_folder_path = effectifs_root_path / Path(effectifs_folder_name_alias)\n",
    "all_effectifs_path    = effectifs_folder_path / Path(effectifs_file_name_alias)\n",
    "hal_all_effectifs_path = effectifs_folder_path / Path(\"Hal_\" + effectifs_file_name_alias)\n",
    "\n",
    "useful_col_list = list(eg.EMPLOYEES_USEFUL_COLS.values()) + list(eg.EMPLOYEES_ADD_COLS.values())\n",
    "hal_useful_col_list = useful_col_list + [full_name_col_alias]\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a3859f-7594-4de2-8851-668f491ebaa2",
   "metadata": {},
   "source": [
    "## 1- Configuring employees database for HalConf\n",
    "independant of year_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfdae0-a256-40af-a28e-1250f52b3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_sheets_to_workbook(file_full_path, df_to_add, sheet_name):\n",
    "    \"\"\"Adds the dataframe 'df_to_add' as sheet named 'sheet_name' \n",
    "    to the existing Excel file with full path 'file_full_path'. \n",
    "\n",
    "    If the sheet name already exists it is overwritten by the new one.\n",
    "    \"\"\"\n",
    "\n",
    "    with pd.ExcelWriter(file_full_path,  # https://github.com/PyCQA/pylint/issues/3060 pylint: disable=abstract-class-instantiated\n",
    "                        engine = 'openpyxl',\n",
    "                        mode = 'a',\n",
    "                        if_sheet_exists = 'replace') as writer:\n",
    "        df_to_add.to_excel(writer, sheet_name = sheet_name, index = False)\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ccca4-a3ae-4701-9c07-d89a1fbacd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_employees_data_hal(all_effectifs_path, search_depth_init, useful_col_list, corpus_year=None):\n",
    "    \"\"\"Sets Intitute employees data.\n",
    "\n",
    "    This is done through the `update_employees` function imported from \n",
    "    `bmfuncts.update_employees` module after check of available \n",
    "    files for update (should be single) and check of Institute employees \n",
    "    database file.\n",
    "\n",
    "    Args:\n",
    "        all_effectifs_path (path): Full path to file of Institute employees database.\n",
    "        search_depth (int): Initial search depth.\n",
    "        useful_col_list (list): Columns to read in the file of Institute employees database.\n",
    "        corpus_year (str): Optional corpus year defined by 4 digits (default: None).\n",
    "        progress_callback (function): Function for updating \\\n",
    "        ProgressBar tkinter widget status.\n",
    "    Returns:\n",
    "        (tup): (employees data (df), adapted search depth (int), \\\n",
    "        list of available years of employees data).    \n",
    "    \"\"\"\n",
    "\n",
    "    # Getting employees df\n",
    "    useful_col_list = list(eg.EMPLOYEES_USEFUL_COLS.values()) + list(eg.EMPLOYEES_ADD_COLS.values())\n",
    "    all_effectifs_df = pd.read_excel(all_effectifs_path,\n",
    "                                     sheet_name = None,\n",
    "                                     dtype = eg.EMPLOYEES_COL_TYPES,\n",
    "                                     usecols = useful_col_list,\n",
    "                                     converters=eg.EMPLOYEES_CONVERTERS_DIC)\n",
    "\n",
    "    # Identifying available years in employees df\n",
    "    annees_dispo = [int(x) for x in list(all_effectifs_df.keys())]\n",
    "    annees_verifiees = annees_dispo\n",
    "    search_depth = search_depth_init\n",
    "    if corpus_year:        \n",
    "        annees_a_verifier = [int(corpus_year) - int(search_depth)\n",
    "                             + (i+1) for i in range(int(search_depth))]\n",
    "        annees_verifiees = []\n",
    "        for i in annees_a_verifier:\n",
    "            if i in annees_dispo:\n",
    "                annees_verifiees.append(i)\n",
    "\n",
    "        if len(annees_verifiees) > 0:\n",
    "            search_depth = min(int(search_depth), len(annees_verifiees))\n",
    "        else:\n",
    "            search_depth = 0\n",
    "            warning_title = \"!!! Attention !!!\"\n",
    "            warning_text  = (\"Le nombre d'années disponibles est insuffisant \"\n",
    "                             \"dans le fichier des effectifs de l'Institut.\"\n",
    "                             \"\\nLe croisement auteurs-effectifs ne peut être effectué !\"\n",
    "                             \"\\n1- Complétez le fichier des effectifs de l'Institut ;\"\n",
    "                             \"\\n2- Puis relancer le croisement auteurs-effectifs.\")\n",
    "            messagebox.showwarning(warning_title, warning_text)\n",
    "    return (all_effectifs_df, search_depth, annees_verifiees)\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2006cc1-8963-48a7-ac02-9560f8c5a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _capitalize_name(name):\n",
    "    name_space_split = name.split(\" \")\n",
    "    name_cap_list = []\n",
    "    for sub_name in name_space_split:\n",
    "        sub_name_minus_split = [x.capitalize() for x in sub_name.split(\"-\")]\n",
    "        sub_name_cap = \"-\".join(sub_name_minus_split)\n",
    "        name_cap_list.append(sub_name_cap)\n",
    "    name_cap = \" \".join(name_cap_list)\n",
    "    return name_cap\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1fde9-ab70-4596-8805-9c21835208fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building HAL employees database\n",
    "tup = _set_employees_data_hal(all_effectifs_path, search_depth_init_alias, useful_col_list)\n",
    "all_effectifs_df, search_depth, annees_disponibles = tup[0], tup[1], tup[2]\n",
    "\n",
    "# Setting effectif full name with full first name\n",
    "sheet_init = True\n",
    "for annee in all_effectifs_df.keys():\n",
    "    annee_all_effectifs_df = all_effectifs_df[str(annee)].copy()\n",
    "    hal_annee_all_effectifs_df = pd.DataFrame()\n",
    "    for _,row in annee_all_effectifs_df.iterrows():\n",
    "        first_name_cap = _capitalize_name(row[first_name_col_alias])\n",
    "        last_name_cap = _capitalize_name(row[last_name_col_alias])\n",
    "        row[full_name_col_alias] = first_name_cap + \" \" + last_name_cap\n",
    "        hal_annee_all_effectifs_df = pd.concat([hal_annee_all_effectifs_df, row.to_frame().T])\n",
    "    sheet_name = str(annee)\n",
    "    if sheet_init:\n",
    "        hal_annee_all_effectifs_df.to_excel(hal_all_effectifs_path, sheet_name=sheet_name, index=False)\n",
    "        sheet_init = False\n",
    "    else:\n",
    "        _add_sheets_to_workbook(hal_all_effectifs_path, hal_annee_all_effectifs_df, sheet_name)\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e63d31-6173-42b9-94f9-ebc9451f6bf3",
   "metadata": {},
   "source": [
    "## 2- Reading the employees database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46137b53-7c37-43a6-9243-d491a9767163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the following functions: _set_employees_data_hal\n",
    "\n",
    "tup = _set_employees_data_hal(hal_all_effectifs_path, search_depth_init_alias,\n",
    "                              hal_useful_col_list, year_select)\n",
    "hal_all_effectifs_df, search_depth, annees_disponibles = tup[0], tup[1], tup[2]\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647e73c-d817-4cfc-a040-326d265f0888",
   "metadata": {},
   "source": [
    "## 3- Reading a clean HAL conferences list\n",
    "L'extraction HAL se fait via HalApyJson en exécutant la cellule ad_hoc de \"Demo_HalApyJson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9476dd5-7117-4387-86d9-4f755e6eb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_database_hal(hal_file_path, iso_country_file_path):\n",
    "    \"\"\"\n",
    "    To Do: Define global parameters\n",
    "    \"\"\"\n",
    "    # Setting useful aliases\n",
    "    unknown_alias = bp.UNKNOWN\n",
    "    \n",
    "    # Getting ISO code-country\n",
    "    code_country_df= pd.read_excel(iso_country_file_path, sheet_name=\"Base\")\n",
    "    code_country_dict = dict(zip(code_country_df[\"Code\"], code_country_df[\"English name\"]))\n",
    "\n",
    "    hal_full_df = pd.read_excel(hal_file_path)\n",
    "\n",
    "    hal_col_list = ['Auteurs', 'Titres', 'Date de publication', 'Journal', 'e-ISSN', 'ISSN',\n",
    "                    'Conference', 'Date de conference', 'Comite de lecture','Acte de conference',\n",
    "                    'Affiliations', 'Institutions', 'Depts', 'Organismes',\n",
    "                    'Type de document', 'Mots clefs', 'DOI', 'Lien url', 'Pays']\n",
    "    hal_col_select_df = hal_full_df[hal_col_list]\n",
    "    hal_comm_df = hal_col_select_df[hal_col_select_df['Type de document']=='COMM']\n",
    "    hal_poster_df = hal_col_select_df[hal_col_select_df['Type de document']=='POSTER']\n",
    "    hal_conf_df = pd.concat([hal_comm_df, hal_poster_df])\n",
    "\n",
    "    clean_hal_conf_df = hal_conf_df.copy()\n",
    "    clean_hal_conf_df.fillna(0, inplace=True)\n",
    "    clean_hal_conf_df.replace(to_replace=0, value=unknown_alias, inplace=True)\n",
    "    clean_hal_conf_df.reindex()\n",
    "    new_hal_conf_df = pd.DataFrame(columns = hal_col_list)\n",
    "    pub_id = 0\n",
    "    for _, row in clean_hal_conf_df.iterrows():\n",
    "        row[\"Pub_id\"] = pub_id\n",
    "        country_iso = str(row[\"Pays\"]).upper()\n",
    "        row[\"Pays\"] = code_country_dict[country_iso]\n",
    "        authors_list = row['Auteurs'].split(\",\")\n",
    "        auth_id = 0\n",
    "        for author in authors_list:\n",
    "            row[\"Auteur_id\"] = auth_id\n",
    "            row[\"Co_auteur Liten\"] = author\n",
    "            row[\"Premier auteur\"] = authors_list[0]\n",
    "            row[\"Année de conférence\"] = row['Date de conference'][0:4]\n",
    "            row[\"Année de publication\"] = row['Date de publication'][0:4]\n",
    "            new_hal_conf_df = pd.concat([new_hal_conf_df, row.to_frame().T])\n",
    "            auth_id += 1\n",
    "        pub_id += 1\n",
    "    order_col_list = [\"Pub_id\", \"Auteur_id\", \"Co_auteur Liten\", \"Premier auteur\",\n",
    "                      \"Année de publication\", \"Année de conférence\", 'Conference', 'Pays',\n",
    "                      'Type de document', 'Comite de lecture','Titres', 'DOI',\n",
    "                      'Mots clefs', 'Acte de conference', 'Lien url',\n",
    "                      'Date de conference', 'Date de publication', \n",
    "                      'Auteurs', 'Affiliations', 'Institutions', 'Depts', 'Organismes'\n",
    "                    ]\n",
    "    final_hal_conf_df = new_hal_conf_df[order_col_list]\n",
    "    \n",
    "    return final_hal_conf_df\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b36c3-7b14-4693-b3e6-81d0af5d7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and saving a clean HAL conferences list\n",
    "\n",
    "# Setting specific filenames and paths independant from year_select\n",
    "iso_country_file = \"Code ISO.xlsx\"\n",
    "hal_country_path = hal_path / Path(\"Pays\")\n",
    "iso_country_file_path = hal_country_path / Path(iso_country_file)\n",
    "\n",
    "# Setting specific filenames dependent on year_select\n",
    "hal_file = f\"{year_select} HAL.xlsx\"\n",
    "hal_split_file = \"Conf_split_\" + hal_file\n",
    "\n",
    "# Setting specific paths dependent on year_select\n",
    "year_hal_path = hal_path / Path(year_select)\n",
    "hal_corpus_path = year_hal_path / Path(\"HAL corpus\")\n",
    "hal_file_path = hal_corpus_path / Path(hal_file)\n",
    "hal_conf_file_path = hal_corpus_path / Path(hal_split_file)\n",
    "\n",
    "# Reading and saving the built HAL conferences list\n",
    "hal_conf_df = read_database_hal(hal_file_path, iso_country_file_path)\n",
    "hal_conf_df.to_excel(hal_conf_file_path, index=False)\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f11686-31bb-4144-ac06-6cf9239bc0ba",
   "metadata": {},
   "source": [
    "## 3- Merging HAL conferences list with employees database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0163d-a6f9-4c14-9f93-ae5e06cde304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from bmfuncts.config_utils import set_org_params\n",
    "\n",
    "org_tup = set_org_params(institute, hal_path)\n",
    "\n",
    "# Setting useful aliases\n",
    "orphan_treat_root = pg.ARCHI_ORPHAN[\"root\"]\n",
    "adds_file_name_alias = pg.ARCHI_ORPHAN[\"employees adds file\"]\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b18960e-35ae-4dac-a987-3c9454334a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _standardize(text):\n",
    "    # Removing accentuated characters\n",
    "    new_text = bp.remove_special_symbol(text, only_ascii=True, strip=True)\n",
    "    return new_text\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebd04e-885b-4490-b5e0-632ed5d28cad",
   "metadata": {},
   "source": [
    "### Checking author names spelling and modifying HAL extraction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f13d76-bd1b-4c9c-91b2-2480b9b8757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the functions: _standardize, _capitalize_name\n",
    "\n",
    "def _check_names_spelling_hal(hal_path, init_df, pub_fullname_col, verbose=None):\n",
    "    \"\"\"Replace author names in 'init_df' dataframe by the employee name.\n",
    "\n",
    "    This is done when a name-spelling discrepency is given in the dedicated \n",
    "    Excel file named 'orthograph_file_name' and located in the 'orphan_treat_root' \n",
    "    folder of the working folder.\n",
    "\n",
    "    Args:\n",
    "        hal_path (path): Full path to working folder.\n",
    "        init_df (dataframe): Publications list with one row per author \\\n",
    "        where author names should be corrected.\n",
    "        cols_tup (tup): Tuple of useful column names in 'init_df' dataframe \\\n",
    "        = (full name, last name, first name).\n",
    "    Returns:\n",
    "        (dataframe): Publications list with one row per author where \\\n",
    "        spelling of author names have been corrected.\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting useful aliases\n",
    "    orphan_treat_root    = pg.ARCHI_ORPHAN[\"root\"]\n",
    "    orthograph_file_name = pg.ARCHI_ORPHAN[\"orthograph file\"]\n",
    "    \n",
    "    # Setting useful column names\n",
    "    ortho_fullname_init_col = \"Nom pub complet\"\n",
    "    ortho_fullname_new_col = \"Nom eff complet\"\n",
    "\n",
    "    # Setting useful path\n",
    "    ortho_path = hal_path / Path(orphan_treat_root) / Path(orthograph_file_name)\n",
    "\n",
    "    # Reading data file targeted by 'ortho_path'\n",
    "    ortho_col_list = [ortho_fullname_init_col,\n",
    "                      ortho_fullname_new_col]\n",
    "    warnings.simplefilter(action = 'ignore', category = UserWarning)\n",
    "    ortho_df = pd.read_excel(ortho_path, usecols = ortho_col_list)\n",
    "\n",
    "    new_df = init_df.copy()\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "    new_df[pub_fullname_col] = new_df[pub_fullname_col].apply(lambda x: _standardize(x))\n",
    "    for pub_row_num, pub_row in new_df.iterrows():\n",
    "        fullname_init = new_df.loc[pub_row_num, pub_fullname_col].lower()\n",
    "        for ortho_row_num, ortho_row in ortho_df.iterrows():\n",
    "            fullname_pub_ortho = ortho_df.loc[ortho_row_num, ortho_fullname_init_col].lower()\n",
    "            fullname_empl_ortho = ortho_df.loc[ortho_row_num, ortho_fullname_new_col]\n",
    "            if fullname_init == fullname_pub_ortho:                \n",
    "                fullname_empl_ortho = ortho_df.loc[ortho_row_num, ortho_fullname_new_col]\n",
    "                names_list = []\n",
    "                for name in fullname_empl_ortho.split(\" \"):\n",
    "                    names_list.append(_capitalize_name(name))\n",
    "                fullname_new = \" \".join(names_list)\n",
    "                new_df.loc[pub_row_num, pub_fullname_col] = fullname_new\n",
    "                if verbose:\n",
    "                    if fullname_init==\"Minh-Nhut Ngo\".lower() and \"ngo\" in fullname_pub_ortho:\n",
    "                        print(\"\\nfullname_init:\", fullname_init)\n",
    "                        print(\"fullname_pub_ortho:\", fullname_pub_ortho)\n",
    "                        print(\"fullname_empl_ortho:\", fullname_empl_ortho)\n",
    "                        print('fullname_new:', fullname_new)\n",
    "    return new_df\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469595c7-449d-467b-90c2-d162a1c1346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking author names spelling and modifying HAL extraction file\n",
    "\n",
    "# Setting specific file names\n",
    "mod_hal_file = \"mod_\" + hal_file\n",
    "\n",
    "# Setting specific paths\n",
    "mod_hal_path = hal_corpus_path / Path(mod_hal_file)\n",
    "\n",
    "# Setting specific aliases\n",
    "co_author_alias = \"Co_auteur Liten\"\n",
    "\n",
    "mod_hal_conf_df = _check_names_spelling_hal(hal_path, hal_conf_df, co_author_alias, verbose=False)\n",
    "mod_hal_conf_df.to_excel(mod_hal_path, index=False)\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795e105-3b1c-4417-8fee-346693b8a385",
   "metadata": {},
   "source": [
    "### Merging with employees database and external PhD students database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec1a73-6ece-49cf-a9f6-0f2b944d4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_ext_docs_hal(init_submit_df, init_orphan_df, ext_docs_path,\n",
    "                      co_author_join_col, ext_docs_full_name_col,\n",
    "                      verbose=False):\n",
    "    # Setting sheet name to read in complementary employees file\n",
    "    ext_docs_sheet_name_alias = pg.SHEET_NAMES_ORPHAN[\"docs to add\"]\n",
    "\n",
    "    # Setting useful aliases\n",
    "    converters_alias = eg.EMPLOYEES_CONVERTERS_DIC\n",
    "    firstname_initials_col_alias = eg.EMPLOYEES_ADD_COLS['first_name_initials']\n",
    "\n",
    "    # Setting useful column names\n",
    "    ext_docs_useful_col_list = eg.EXT_DOCS_USEFUL_COL_LIST.copy()\n",
    "    col_name_to_change = ext_docs_useful_col_list[-2]\n",
    "    ext_docs_useful_col_list[-2] = col_name_to_change[:-2]\n",
    "\n",
    "    # Reading of the external phd students excel file\n",
    "    # using the same useful columns as init_submit_df defined by EXT_DOCS_USEFUL_COL_LIST\n",
    "    # with dates conversion through converters_alias\n",
    "    # and drop of empty rows\n",
    "    warnings.simplefilter(action = 'ignore', category = UserWarning)\n",
    "    ext_docs_df = pd.read_excel(ext_docs_path,\n",
    "                                sheet_name=ext_docs_sheet_name_alias,\n",
    "                                usecols=ext_docs_useful_col_list,\n",
    "                                converters=converters_alias)\n",
    "    ext_docs_df.rename({firstname_initials_col_alias: firstname_initials_col_alias}, inplace=True)\n",
    "    ext_docs_df.dropna(how='all', inplace=True)\n",
    "    ext_docs_df.reset_index(drop=True, inplace=True)\n",
    "    ext_docs_df[co_author_join_col] = ext_docs_df[ext_docs_full_name_col].apply(lambda x: _standardize(x).lower().replace(\"-\",\" \"))\n",
    "\n",
    "#    # for printing intermediate stages\n",
    "#    # Initializing new_submit_df as copy of init_submit_df    \n",
    "#    new_submit_df = init_submit_df.copy()\n",
    "#    \n",
    "#    co_author_col = \"Co_auteur Liten\"\n",
    "#    for orph_idx, orph_row in init_orphan_df.iterrows():\n",
    "#        orphan_name = init_orphan_df.loc[orph_idx, co_author_col]\n",
    "#        orphan_name_join = init_orphan_df.loc[orph_idx, co_author_join_col]\n",
    "#            \n",
    "#        for edoc_idx, edoc_row in ext_docs_df.iterrows():\n",
    "#            edoc_name = ext_docs_df.loc[edoc_idx, ext_docs_full_name_col]\n",
    "#            edoc_name_join = ext_docs_df.loc[edoc_idx, co_author_join_col]\n",
    "#\n",
    "#            if orphan_name_join==edoc_name_join:\n",
    "#                if \"yefsah\" in edoc_name_join and verbose: print(\"\\n\",orphan_name_join)\n",
    "#                orph_row_df = orph_row.to_frame().T\n",
    "#                edoc_row_df = edoc_row.to_frame().T\n",
    "#                submit_adds_df = orph_row_df.merge(edoc_row_df, how='inner', on=co_author_join_col)\n",
    "#                new_submit_df = pd.concat([new_submit_df, submit_adds_df])\n",
    "#                \n",
    "#                if verbose:\n",
    "#                    if orphan_name_join==\"Lydia YEFSAH\".lower() or \"yefsah\" in edoc_name:\n",
    "#                        print(\"\\norphan_name:\", orphan_name)\n",
    "#                        print(\"orphan_name_join:\", orphan_name_join)\n",
    "#                        print(\"edoc_name:\", edoc_name)\n",
    "#                        print('edoc_name_join:', edoc_name_join)\n",
    "\n",
    "    submit_adds_df = init_orphan_df.merge(ext_docs_df, how='inner', on=co_author_join_col)\n",
    "    new_submit_df = pd.concat([init_submit_df, submit_adds_df])\n",
    "    new_submit_df.sort_values(by=[\"Pub_id\", \"Auteur_id\"], inplace=True)\n",
    "    to_drop_df = new_submit_df[init_orphan_df.columns]\n",
    "    new_orphan_df = pd.concat([init_orphan_df, to_drop_df, to_drop_df]).drop_duplicates(keep=False)\n",
    "\n",
    "    return new_submit_df, new_orphan_df\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee442172-e356-4c98-9bfa-c180884eef7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting specific aliases\n",
    "co_author_join_alias = \"co_author_join\"\n",
    "pub_id_alias = \"Pub_id\"\n",
    "author_id_alias = \"Auteur_id\"\n",
    "\n",
    "# Setting specific file and folder names\n",
    "hal_submit_file = \"submit_final.xlsx\"\n",
    "hal_orphan_file = \"orphan_final.xlsx\"\n",
    "bdd_multi_mensuelle_folder = \"0 - BDD multi mensuelle\"\n",
    "\n",
    "# Setting specific paths independant from year_select\n",
    "hal_ext_docs_path = hal_path / Path(orphan_treat_root) / Path(adds_file_name_alias)\n",
    "\n",
    "# Setting specific paths dependent on year_select\n",
    "hal_submit_path = year_hal_path / Path(bdd_multi_mensuelle_folder)\n",
    "hal_orphan_path = year_hal_path / Path(bdd_multi_mensuelle_folder)\n",
    "hal_submit_file_path = hal_submit_path / Path(hal_submit_file)\n",
    "hal_orphan_file_path = hal_orphan_path / Path(hal_orphan_file)\n",
    "\n",
    "# Initializing orphan df through standardization of co-authors name\n",
    "orphan_df = mod_hal_conf_df.copy()\n",
    "orphan_df[co_author_join_alias] = mod_hal_conf_df[co_author_alias].apply(lambda x: _standardize(x).lower().replace(\"-\",\" \"))\n",
    "print(len(orphan_df))\n",
    "\n",
    "# Building the search time depth of Institute co-authors among the employees dataframe\n",
    "corpus_year_status = year_select in hal_all_effectifs_df.keys()\n",
    "year_start = int(year_select)\n",
    "if not corpus_year_status:\n",
    "    year_start = int(year_select)-1\n",
    "year_stop = year_start - (search_depth - 1)\n",
    "years = [str(i) for i in range(year_start, year_stop-1,-1)]\n",
    "\n",
    "#years = [\"2022\"]\n",
    "submit_df = pd.DataFrame()\n",
    "first_step = True\n",
    "for empl_year in years:\n",
    "    empl_df = hal_all_effectifs_df[empl_year].copy()\n",
    "    empl_df[co_author_join_alias] = empl_df[full_name_col_alias].apply(lambda x: _standardize(x).lower().replace(\"-\",\" \"))\n",
    "    \n",
    "#    # for printing intermediate stages\n",
    "#    hal_pub_empl_df = pd.DataFrame()\n",
    "#    for orph_idx, orph_row in orphan_df.iterrows():\n",
    "#        orphan_name = orphan_df.loc[orph_idx, co_author_alias]\n",
    "#        orphan_name_join = orphan_df.loc[orph_idx, co_author_join_alias]\n",
    "#            \n",
    "#        for empl_idx, empl_row in empl_df.iterrows():\n",
    "#            empl_name = empl_df.loc[empl_idx, full_name_col_alias]\n",
    "#            empl_name_join = empl_df.loc[empl_idx, co_author_join_alias]\n",
    "#                \n",
    "#            if orphan_name_join==empl_name_join:\n",
    "#                orph_row_df = orph_row.to_frame().T\n",
    "#                empl_row_df = empl_row.to_frame().T\n",
    "#                orphan_empl_df = orph_row_df.merge(empl_row_df, how='inner', on=co_author_join_alias)\n",
    "#                hal_pub_empl_df = pd.concat([hal_pub_empl_df, orphan_empl_df])\n",
    "#                \n",
    "#                if \"vito\" in orphan_name.lower() and \"vito\" in empl_name.lower():\n",
    "#                    print(\"\\norphan_name:\", orphan_name)\n",
    "#                    print(\"orphan_name_join:\", orphan_name_join)\n",
    "#                    print(\"empl_name:\", empl_name)\n",
    "#                    print('empl_name_join:', empl_name_join)\n",
    "#\n",
    "#    hal_merge_file = empl_year + \"_concat_\" + hal_file\n",
    "#    hal_merge_path = hal_corpus_path / Path(hal_merge_file)\n",
    "#    hal_pub_empl_df.to_excel(hal_merge_path, index=False)                \n",
    "\n",
    "    submit_adds_df = orphan_df.merge(empl_df, how='inner', on=co_author_join_alias)\n",
    "    submit_df = pd.concat([submit_df, submit_adds_df])\n",
    "    submit_df.sort_values(by=[pub_id_alias, author_id_alias], inplace=True)\n",
    "    to_drop_df = submit_df[orphan_df.columns]\n",
    "    orphan_df = pd.concat([orphan_df, to_drop_df, to_drop_df]).drop_duplicates(keep=False)\n",
    "   \n",
    "    if first_step:\n",
    "        first_step = False\n",
    "        submit_df, orphan_df = _add_ext_docs_hal(submit_df, orphan_df, hal_ext_docs_path,\n",
    "                                                 co_author_join_alias, full_name_col_alias, verbose=False)  \n",
    "\n",
    "submit_df.to_excel(hal_submit_file_path, index=False)\n",
    "orphan_df.to_excel(hal_orphan_file_path, index=False)\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5395b50e-227a-41e1-be5c-a261f73f122e",
   "metadata": {},
   "source": [
    "### Adding author job type and saving new 'submit_df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf70d8-31f4-4a96-8a4d-7dfc2f52aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_author_job_type_hal(in_path, out_path):\n",
    "    \"\"\"Adds a new column containing the job type for each author \n",
    "    of the publications list with one row per author.\n",
    "\n",
    "    The job type is got from the employee information available \n",
    "    in 3 columns which names are given by 'category_col_alias', \n",
    "    'status_col_alias' and 'qualification_col_alias'. \n",
    "    The name of the new column is given by 'author_type_col_alias'. \n",
    "    The updated publications list is saved as Excel file.\n",
    "\n",
    "    Args:\n",
    "        in_path (path):  Full path to the Excel file of the publications list \\\n",
    "        with one row per author with attributes as Institute employee.\n",
    "        out_path (path): Full path for saving the modified publications list.\n",
    "    Returns:\n",
    "        (str): End message recalling the full path to the saved file of \\\n",
    "        the modified publications list.\n",
    "    \"\"\"\n",
    "\n",
    "    # internal functions:\n",
    "    def _get_author_type(row):\n",
    "        author_type = '-'\n",
    "        for col_name, dic in author_types_dic.items():\n",
    "            for key,values_list in dic.items():\n",
    "                values_status = [True for value in values_list if value in row[col_name]]\n",
    "                if any(values_status):\n",
    "                    author_type = key\n",
    "        return author_type\n",
    "\n",
    "    # Setting useful aliases\n",
    "    category_col_alias      = eg.EMPLOYEES_USEFUL_COLS['category']\n",
    "    status_col_alias        = eg.EMPLOYEES_USEFUL_COLS['status']\n",
    "    qualification_col_alias = eg.EMPLOYEES_USEFUL_COLS['qualification']\n",
    "    author_type_col_alias   = pg.COL_NAMES_BONUS['author_type']\n",
    "\n",
    "    author_types_dic = {category_col_alias      : eg.CATEGORIES_DIC,\n",
    "                        status_col_alias        : eg.STATUS_DIC,\n",
    "                        qualification_col_alias : eg.QUALIFICATION_DIC}\n",
    "\n",
    "    # Read of the excel file\n",
    "    submit_df = pd.read_excel(in_path)\n",
    "\n",
    "    submit_df[author_type_col_alias] = submit_df.apply(_get_author_type, axis=1)\n",
    "\n",
    "    submit_df.to_excel(out_path, index = False)\n",
    "\n",
    "    end_message = f\"Column with author job type added in file: \\n  '{out_path}'\"\n",
    "    return end_message\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a003f-1a13-41e9-a427-9f2f44348f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding author job type and saving new submit_df\n",
    "_add_author_job_type_hal(hal_submit_file_path, hal_submit_file_path)\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e2a4e-f075-43a6-8a5a-921748eccf80",
   "metadata": {},
   "source": [
    "### Adding full article reference and saving new submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cedb89-5d77-429f-a724-48e3d5dfa7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_full_ref_hal(title, first_author, conf_name, conf_year, conf_country, pub_year, doi):\n",
    "    \"\"\"Builds the full reference of a publication.\n",
    "\n",
    "    Args:\n",
    "        title (str): Title of the publication.\n",
    "        first_author (str): First author of the publication formated as 'NAME IJ' \\\n",
    "        with 'NAME' the lastname and 'IJ' the initials of the firstname of the author.\n",
    "        journal_name (str): Name of the journal where the publication is published.\n",
    "        pub_year (str): Publication year defined by 4 digits.\n",
    "        doi (str): Digital identification of the publication.\n",
    "    Returns:\n",
    "        (str): Full reference of the publication.\n",
    "    \"\"\"\n",
    "    full_ref  = f'{title}, '                     # add the reference's title\n",
    "    full_ref += f'{first_author}. et al., '      # add the reference's first author\n",
    "    full_ref += f'{conf_name}, '                 # add the reference's conference name\n",
    "    full_ref += f'{conf_country}-{conf_year}, '       # add the reference's conference country and year\n",
    "    full_ref += f'{pub_year}'                  # add the reference's publication year\n",
    "    full_ref += f'{doi}'                         # add the reference's DOI if available\n",
    "    return full_ref\n",
    "\n",
    "\n",
    "def _add_biblio_list_hal(in_path, out_path):\n",
    "    \"\"\"Adds a new column containing the full reference of each publication \n",
    "    of the publications list with one row per author.\n",
    "\n",
    "    The full reference is built by concatenating the folowing items: \n",
    "    title, first author, year, journal, DOI. \n",
    "    These items are got from the columns which names are given by \n",
    "    'pub_title_alias', 'pub_first_author_alias', 'pub_year_alias', \n",
    "    'pub_journal_alias' and 'pub_doi_alias', respectively. \n",
    "    The name of the new column is given by 'pub_full_ref_alias'. \n",
    "    The updated publications list is saved as Excel file.\n",
    "\n",
    "    Args:\n",
    "        in_path (path): Full path to the Excel file of the publications list.\n",
    "        out_path (path): Full path for saving the modified publications list.\n",
    "    Returns:\n",
    "        (str): End message recalling the full path to the saved file \\\n",
    "        of the modified publications list.\n",
    "    \"\"\"\n",
    "    # Setting useful aliases\n",
    "    unknown_alias = bp.UNKNOWN\n",
    "    pub_id_alias           = 'Pub_id'\n",
    "    pub_first_author_alias = \"Premier auteur\"\n",
    "    pub_year_alias         = \"Année de publication\"\n",
    "    pub_year_conf_alias    = \"Année de conférence\"\n",
    "    pub_conf_alias         = \"Conference\"\n",
    "    pub_country_alias      = \"Pays\"\n",
    "    pub_doi_alias          = \"DOI\"\n",
    "    pub_title_alias        = \"Titres\"\n",
    "    pub_full_ref_alias     = pg.COL_NAMES_BONUS['liste biblio']\n",
    "\n",
    "    # Read of the excel file\n",
    "    submit_df = pd.read_excel(in_path)\n",
    "\n",
    "    conf_plus_full_ref_df = pd.DataFrame()\n",
    "    # Splitting the frame into subframes with same Pub_id\n",
    "    for _, pub_id_df in submit_df.groupby(pub_id_alias):\n",
    "        # Select the first row and build the full reference\n",
    "        pub_id_first_row = pub_id_df.iloc[0]\n",
    "        title        = str(pub_id_first_row[pub_title_alias])\n",
    "        first_author = str(pub_id_first_row[pub_first_author_alias])\n",
    "        conf_name    = str(pub_id_first_row[pub_conf_alias])\n",
    "        conf_year    = str(pub_id_first_row[pub_year_conf_alias])\n",
    "        conf_country = str(pub_id_first_row[pub_country_alias])\n",
    "        pub_year     = str(pub_id_first_row[pub_year_alias])\n",
    "        doi = \"\"\n",
    "        if pub_id_first_row[pub_doi_alias]!=unknown_alias:\n",
    "            doi = \", \" + str(pub_id_first_row[pub_doi_alias])\n",
    "        pub_id_df[pub_full_ref_alias] = _set_full_ref_hal(title, first_author, conf_name,\n",
    "                                                          conf_year, conf_country, pub_year, doi)\n",
    "        conf_plus_full_ref_df = pd.concat([conf_plus_full_ref_df, pub_id_df])\n",
    "\n",
    "    conf_plus_full_ref_df.to_excel(out_path, index = False)\n",
    "\n",
    "    end_message = f\"Column with full reference of conference added in file: \\n  '{out_path}'\"\n",
    "    return end_message\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc652e9-c8e6-41cf-b470-1297b6c3d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding full article reference and saving new submit_df\n",
    "_add_biblio_list_hal(hal_submit_file_path, hal_submit_file_path)\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55863eca-8675-4dd8-8d1d-7412b69c4657",
   "metadata": {},
   "source": [
    "### Adding list of Institute authors with attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51d036-0d96-4599-b702-61511d73a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_authors_name_list_hal(institute, org_tup, in_path, out_path):\n",
    "    \"\"\"Adds two columns to the dataframe got from the Excel file pointed by 'in_path'.\n",
    "\n",
    "    The columns contain respectively the full name of each author as \"NAME, Firstname\" \n",
    "    and the institute co-authors list with attributes of each author in a string as follows:\n",
    "\n",
    "        - \"NAME1, Firstame1 (matricule,job type,department affiliation, \\\n",
    "        service affiliation,laboratoire affiliation);\n",
    "        - NAME2, Firstame2 (matricule,job type,department affiliation, \\\n",
    "        service affiliation,laboratoire affiliation);\n",
    "        - ...\".\n",
    "\n",
    "    Args:\n",
    "        institute (str): The Intitute name.\n",
    "        org_tup (tup): Contains Institute parameters.\n",
    "        in_path (path): Fullpath of the excel file of the publications list \\\n",
    "        with a row per Institute author and their attributes columns.\n",
    "        out_path (path): Fullpath of the processed dataframe as an Excel file \\\n",
    "        saved after going through its treatment.\n",
    "    Returns:\n",
    "        (str): End message recalling out_path.\n",
    "    \"\"\"\n",
    "    # Internal functions\n",
    "    def _get_dpt_key(dpt_raw):\n",
    "        return_key = None\n",
    "        for key, values in dpt_label_dict.items():\n",
    "            if dpt_raw in values:\n",
    "                return_key = key\n",
    "        return return_key\n",
    "\n",
    "    # Setting institute parameters\n",
    "    dpt_label_dict = org_tup[1]\n",
    "\n",
    "    # Setting useful aliases\n",
    "    pub_id_alias         = \"Pub_id\"\n",
    "    idx_authors_alias    = \"Auteur_id\"\n",
    "    nom_alias            = \"Nom\"\n",
    "    prenom_alias         = \"Prénom\"\n",
    "    matricule_alias      = \"Matricule\"\n",
    "    full_name_alias      = \"Nom complet\"\n",
    "    author_type_alias    = \"Type de l'auteur\"\n",
    "    full_name_list_alias = \"Liste ordonnée des auteurs Liten\"\n",
    "    dept_alias           = \"Dpt/DOB (lib court)\"\n",
    "    serv_alias           = \"Service (lib court)\"\n",
    "    lab_alias            = \"Laboratoire (lib court)\"\n",
    "\n",
    "    # Reading the excel file\n",
    "    df_in = pd.read_excel(in_path)\n",
    "\n",
    "    # Adding the column 'full_name_alias' that will be used to create the authors fullname list\n",
    "    df_in[prenom_alias]    = df_in[prenom_alias].apply(lambda x: x.capitalize())\n",
    "    df_in[full_name_alias] = df_in[nom_alias] + ', ' + df_in[prenom_alias]\n",
    "\n",
    "    df_out = pd.DataFrame()\n",
    "    for _, pub_id_df in df_in.groupby(pub_id_alias):\n",
    "        raw_depts_list = pub_id_df[dept_alias].to_list()\n",
    "        depts_list = [_get_dpt_key(x) for x in raw_depts_list]\n",
    "        for dept in dpt_label_dict.keys():\n",
    "            pub_id_df[dept] = 0\n",
    "            if dept in depts_list:\n",
    "                pub_id_df[dept] = 1\n",
    "        \n",
    "        authors_tup_list = sorted(list(set(zip(pub_id_df[idx_authors_alias],\n",
    "                                               pub_id_df[full_name_alias],\n",
    "                                               pub_id_df[matricule_alias],\n",
    "                                               pub_id_df[author_type_alias],\n",
    "                                               pub_id_df[dept_alias],\n",
    "                                               pub_id_df[serv_alias],\n",
    "                                               pub_id_df[lab_alias]))))\n",
    "\n",
    "        authors_str_list = [(f'{x[1]} ({x[2]},'\n",
    "                             f'{x[3]},{_get_dpt_key(x[4])},{x[5]},{x[6]})')\n",
    "                            for x in authors_tup_list]\n",
    "        authors_full_str = \"; \".join(authors_str_list)\n",
    "        pub_id_df[full_name_list_alias] = authors_full_str\n",
    "\n",
    "        df_out = pd.concat([df_out, pub_id_df])\n",
    "\n",
    "    # Saving 'df_out' in an excel file 'out_path'\n",
    "    df_out.to_excel(out_path, index = False)\n",
    "\n",
    "    end_message = f\"Column with co-authors list is added to the file: \\n  '{out_path}'\"\n",
    "    return end_message, df_out\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531031a2-6e27-440c-b8a0-87917210e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding list of Institute authors with attributes\n",
    "_, submit_df = _add_authors_name_list_hal(institute, org_tup, hal_submit_file_path, hal_submit_file_path)\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349d3c2-4e02-4568-a5de-4c7881d0c93d",
   "metadata": {},
   "source": [
    "### Setting unique pub-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb26c90-ce3e-4553-9114-1cca8178b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unique_pub_id(df, pub_id_col, year_col, shift):\n",
    "    \"\"\"Transforms the column 'Pub_id' of df y adding \"yyyy_\" \n",
    "    (year in 4 digits) to the values.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): data that we want to modify.\n",
    "    Returns:\n",
    "        (pandas.DataFrame): df with its changed column.\n",
    "    \"\"\"\n",
    "    year_df = df[year_col].iloc[0]\n",
    "\n",
    "    def _rename_pub_id(old_pub_id, year):\n",
    "        pub_id_str = str(int(old_pub_id) + shift)\n",
    "        while len(pub_id_str)<3:\n",
    "            pub_id_str = \"0\" + pub_id_str\n",
    "        new_pub_id = str(int(year)) + '_' + pub_id_str\n",
    "        return new_pub_id\n",
    "\n",
    "    df[pub_id_col] = df[pub_id_col].apply(lambda x: _rename_pub_id(x, year_df))\n",
    "    return df\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e8337-e6a4-49d8-b67d-dd5a4eac04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting unique pub-ID\n",
    "shift = 500\n",
    "_unique_pub_id(submit_df, 'Pub_id', 'Année de publication', shift)\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb68ca-b21e-4f77-98b7-aa316b2a0302",
   "metadata": {},
   "source": [
    "### Selecting columns for final conferences list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d25279-695b-4aa8-ab7c-e85d4101f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns for final list\n",
    "final_col_list = ['Pub_id','Année de publication', 'Année de conférence', 'Premier auteur', 'Liste ordonnée des auteurs Liten',\n",
    "                  'Titres', 'Conference', 'Type de document', 'DOI', 'Référence bibliographique complète',\n",
    "                  'Pays', 'DEHT', 'DTCH', 'DTNM', 'DTS', 'DIR', 'Comite de lecture', 'Acte de conference', 'Lien url']\n",
    "sub_submit_df = submit_df[final_col_list]\n",
    "conf_list_df = sub_submit_df.drop_duplicates(['Pub_id', 'Premier auteur', 'Année de conférence', 'Conference', 'Pays',\n",
    "                                              'Type de document', 'Comite de lecture', 'Titres',])\n",
    "final_conf_list_df = conf_list_df.rename(columns={'Titres': 'Titre'})\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52056b7a-6f66-47ee-a5b9-6f9722d864b9",
   "metadata": {},
   "source": [
    "### Formatting and saving final conferences list as openpyxl workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd2e12-5c8b-4ac7-9bb3-cf83767aeeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd party imports\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows \\\n",
    "    as openpyxl_dataframe_to_rows\n",
    "from openpyxl.utils import get_column_letter \\\n",
    "    as openpyxl_get_column_letter\n",
    "from openpyxl.styles import Font as openpyxl_Font\n",
    "from openpyxl.styles import PatternFill as openpyxl_PatternFill\n",
    "from openpyxl.styles import Alignment as openpyxl_Alignment\n",
    "from openpyxl.styles import Border as openpyxl_Border\n",
    "from openpyxl.styles import Side as openpyxl_Side\n",
    "\n",
    "\n",
    "def hal_set_col_attr():\n",
    "    \"\"\"Sets the dict for setting the final column attributes \n",
    "    in terms of width and alignment to be used for formating \n",
    "    datarames before openpyxl save.\n",
    "\n",
    "    The final column names are got through the \n",
    "    `build_col_conversion_dic` internal function.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "    Returns:\n",
    "        (tup): (dict to be used for setting the final column \\\n",
    "        attributes for formating datarames before openpyxl save, \\\n",
    "        list of the final column names that have attributes).\n",
    "    \"\"\"\n",
    "    col_attr = {'Pub_id'                             : [20, \"center\"],\n",
    "                'Année de publication'               : [15, \"center\"],\n",
    "                'Année de conférence'                : [15, \"center\"],\n",
    "                'Premier auteur'                     : [20, \"center\"],\n",
    "                'Liste ordonnée des auteurs Liten'   : [40, \"left\"],\n",
    "                'Titre'                              : [40, \"left\"],\n",
    "                'Conference'                         : [40, \"left\"],\n",
    "                'Type de document'                   : [15, \"center\"],\n",
    "                'DOI'                                : [20, \"center\"],\n",
    "                'Référence bibliographique complète' : [55, 'left'],\n",
    "                'Pays'                               : [15, \"center\"],\n",
    "                'DEHT'                               : [10, \"center\"],\n",
    "                'DTCH'                               : [10, \"center\"],\n",
    "                'DTNM'                               : [10, \"center\"],\n",
    "                'DTS'                                : [10, \"center\"],\n",
    "                'DIR'                                : [10, \"center\"],\n",
    "                'Comite de lecture'                  : [10, \"center\"],\n",
    "                'Acte de conference'                 : [15, \"center\"],\n",
    "                'Lien url'                           : [50, \"left\"]\n",
    "               }\n",
    "    col_set_list = list(col_attr.keys())\n",
    "    col_attr['else'] = [15, \"center\"]\n",
    "    \n",
    "    return col_attr, col_set_list\n",
    "\n",
    "\n",
    "def _hal_mise_en_page(df, wb = None):\n",
    "    \"\"\"Formats a worksheet of an openpyxl workbook using \n",
    "    columns attributes got through the `set_col_attr` function \n",
    "    imported from the `bmfuncts.rename_cols` module.\n",
    "\n",
    "    When the workbook wb is not None, this is applied \n",
    "    to the active worksheet of the passed workbook. \n",
    "    If the workbook wb is None, then the workbook is created.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): The dataframe to be formatted.\n",
    "        wb (openpyxl workbook): Worbook of the worksheet \\\n",
    "        to be formatted (default = None).\n",
    "    Returns:\n",
    "        (tup): (worbook of the formatted worksheet (openpyxl workbook), \\\n",
    "        formatted active sheet).\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting useful column sizes\n",
    "    col_attr, col_set_list = hal_set_col_attr()\n",
    "    columns_list = list(df.columns)\n",
    "    for col in columns_list:\n",
    "        if col not in col_set_list:\n",
    "            col_attr[col] = col_attr['else']\n",
    "\n",
    "    # Setting list of cell colors\n",
    "    cell_colors = [openpyxl_PatternFill(fgColor = pg.ROW_COLORS['odd'], fill_type = \"solid\"),\n",
    "                   openpyxl_PatternFill(fgColor = pg.ROW_COLORS['even'], fill_type = \"solid\")]\n",
    "\n",
    "    # Initialize wb as a workbook and ws its active worksheet\n",
    "    if not wb:\n",
    "        wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws_rows = openpyxl_dataframe_to_rows(df, index=False, header=True)\n",
    "\n",
    "    # Coloring alternately rows in ws using list of cell colors cell_colors\n",
    "    for idx_row, row in enumerate(ws_rows):\n",
    "        ws.append(row)\n",
    "        last_row = ws[ws.max_row]\n",
    "        if idx_row >= 1:\n",
    "            cell_color = cell_colors[idx_row%2]\n",
    "            for cell in last_row:\n",
    "                cell.fill = cell_color\n",
    "\n",
    "    # Setting cell alignment and border using dict of column attributes col_attr\n",
    "    for idx_col, col in enumerate(columns_list):\n",
    "        column_letter = openpyxl_get_column_letter(idx_col + 1)\n",
    "        for cell in ws[column_letter]:\n",
    "            cell.alignment = openpyxl_Alignment(horizontal=col_attr[col][1],\n",
    "                                                vertical=\"center\")\n",
    "            cell.border = openpyxl_Border(left=openpyxl_Side(border_style='thick',\n",
    "                                                             color='FFFFFF'),\n",
    "                                          right=openpyxl_Side(border_style='thick',\n",
    "                                                              color='FFFFFF'))\n",
    "\n",
    "    # Setting the format of the columns heading\n",
    "    cells_list = ws['A'] + ws[1]\n",
    "    for cell in cells_list:\n",
    "        cell.font = openpyxl_Font(bold=True)\n",
    "        cell.alignment = openpyxl_Alignment(wrap_text=True, horizontal=\"center\",\n",
    "                                            vertical=\"center\")\n",
    "\n",
    "    # Setting de columns width using dict \n",
    "    # of column attributes 'col_attr' \n",
    "    for idx_col, col in enumerate(columns_list):\n",
    "        if idx_col >= 1:\n",
    "            column_letter = openpyxl_get_column_letter(idx_col + 1)\n",
    "            if col in col_attr.keys():\n",
    "                ws.column_dimensions[column_letter].width = col_attr[col][0]\n",
    "            else:\n",
    "                ws.column_dimensions[column_letter].width = 20\n",
    "\n",
    "\n",
    "    # Setting height of first row\n",
    "    first_row_num = 1\n",
    "    ws.row_dimensions[first_row_num].height = 50\n",
    "\n",
    "    return wb, ws\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15f669-73de-451e-8b48-426cb70248ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formating and saving final list as workbook\n",
    "wb, _ = _hal_mise_en_page(final_conf_list_df)\n",
    "\n",
    "hal_results_path = year_hal_path / Path(\"3 - Résultats Finaux\")\n",
    "conf_list_path = hal_results_path / Path(\"Liste consolidée \" + year_select + \"_Conférences.xlsx\")\n",
    "wb.save(conf_list_path)\n",
    "\n",
    "print(\"Cell run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4aa35f-d7a3-4683-b6a9-f614c1441dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BiblioMeter",
   "language": "python",
   "name": "bibliometer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
